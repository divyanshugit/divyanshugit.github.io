---
---

@article{Kumar2024Apr,
  abbr={arXiv},
	author = {Kumar, Divyanshu and Kumar, Anurakt and Agarwal, Sahil and Harshangi, Prashanth},
	title = {{Increased LLM Vulnerabilities from Fine-tuning and Quantization}},
	journal = {arXiv},
	year = {2024},
	month = apr,
	eprint = {2404.04392},
	doi = {10.48550/arXiv.2404.04392}
}
@inproceedings{
kumar2024investigating,
abbr={NeurIPS Workshop},
title={Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 {LLM}s},
author={Divyanshu Kumar* and Umang Jain* and Sahil Agarwal and Prashanth Harshangi},
booktitle={Neurips Safe Generative AI Workshop 2024},
year={2024},
month=oct,
url={https://openreview.net/forum?id=tYDn5pGs5P}
}

@inproceedings{
kumar2024sagert,
abbr={NeurIPS Workshop},
title={{SAGE}-{RT}: Synthetic Alignment data Generation for Safety Evaluation and Red Teaming},
author={Anurakt Kumar* and Divyanshu Kumar* and Jatan Loya and Nitin Aravind Birur and Tanay Baswa and Sahil Agarwal and Prashanth Harshangi},
booktitle={Red Teaming GenAI: What Can We Learn from Adversaries?},
year={2024},
month=oct,
url={https://openreview.net/forum?id=ftHy6rA8LL}
}
@inproceedings{
tanay2024efficacy,
abbr={NeurIPS Workshop},
title={Efficacy of the {SAGE}-{RT} Dataset for Model Safety Alignment: A Comparative Study},
author={Tanay Baswa and Nitin Aravind Birur and Divyanshu Kumar and Jatan Loya and Anurakt Kumar and Prashanth Harshangi, Sahil Agarwal},
booktitle={Pluralistic Alignment Workshop at NeurIPS 2024},
year={2024},
month=oct,
url={https://openreview.net/forum?id=wl2vBu8jX4}
}
@misc{birur2024veravalidationenhancementretrieval,
      abbr={arXiv},
	  title={VERA: Validation and Enhancement for Retrieval Augmented systems}, 
      author={Nitin Aravind Birur and Tanay Baswa and Divyanshu Kumar and Jatan Loya and Sahil Agarwal and Prashanth Harshangi},
      year={2024},
	  month=sep,
      eprint={2409.15364},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.15364}, 
}