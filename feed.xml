<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://divyanshugit.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://divyanshugit.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-17T16:15:34+00:00</updated><id>https://divyanshugit.github.io/feed.xml</id><title type="html">Divyanshu Kumar</title><subtitle>I&apos;m a ML Reserach Engineer(Founding Engineer) at Enkrypt AI, where I work on finding vulnerability of Gen AI models and developing tools to make the Gen AI usecase safe and secure. </subtitle><entry><title type="html">DP Guarantee in Action</title><link href="https://divyanshugit.github.io/blog/2025/dp-guarantee-in-action/" rel="alternate" type="text/html" title="DP Guarantee in Action"/><published>2025-03-02T00:00:00+00:00</published><updated>2025-03-02T00:00:00+00:00</updated><id>https://divyanshugit.github.io/blog/2025/dp-guarantee-in-action</id><content type="html" xml:base="https://divyanshugit.github.io/blog/2025/dp-guarantee-in-action/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dp_blog/2-480.webp 480w,/assets/img/dp_blog/2-800.webp 800w,/assets/img/dp_blog/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/dp_blog/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the previous blog<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, we discussed why traditional encryption and anonymization methods often fall short when balancing <strong>privacy and utility</strong>. We also introduced <strong>Differential Privacy</strong> as a promising solution to this challenge. Now, let’s take a deeper dive into understanding how its privacy guarantees actually work:</p> <h2 id="dp-in-action-a-formal-proof">DP in Action: A formal Proof</h2> <p>Differential privacy (DP) is one of the most robust frameworks for ensuring data privacy in machine learning and data analytics. A key strength of DP is that <em>post-processing does not weaken privacy guarantees</em>. In simple terms, if you start with a differentially private mechanism and then apply any additional function to its output, the result remains differentially private.</p> <p>In this one, we will formally prove that if a randomized mechanism \(M\) is (\(\epsilon, \delta\))-differentially private, then any further randomized transformation \(f\) of its output does not degrade its privacy properties.</p> <h2 id="understanding-the-problem-in-hand">Understanding the problem in hand</h2> <p>Let’s define the terms formally:</p> <p>We have a randomized algorithm (also called a mechanism)</p> \[M : \mathbb{N}^{|X|} \to R\] <p>that satisfies (\(\epsilon, \delta\))-differential privacy. This means that for all neighboring datasets \(D\) and \(D'\) (datasets that differ by at most one entry), and for all measurable subsets \(S \subseteq \mathbb{R}\), we have:</p> \[\Pr[(f \circ M)(D) \in S_0] = \int_{\mathcal{R}} \Pr[f(r) \in S_0] \cdot \Pr[M(D) \in dx]\] <p>where \(Pr\) represents the probability measure over the randomness in \(M\).</p> <p>We then define an <em>arbitrary randomized function</em></p> \[f: R \to R_0\] <p>which transforms the output of \(M\). Our goal is to show that the composed function:</p> \[f \circ M : \mathbb{N}^{|X|} \to R_0\] <p>is still (\(\epsilon, \delta\))-differentially private.</p> <h2 id="the-formal-proof">The Formal Proof</h2> <p>In other word, the the composite function can be defined as:</p> \[(f \circ M)(D) = f(M(D))\] <p>which means we first apply \(M\) to \(\) to get an intermediate result in \(\), and then apply \(f\) to transform that result into \(R_0\).</p> <p>We need to show that for any measurable set \(S_0 \subseteq R_0\), the mechanism \(f \circ M\) satisfies:</p> \[\Pr[(f \circ M)(D) \in S_0] \leq e^{\varepsilon} \Pr[(f \circ M)(D') \in S_0] + \delta.\] <p>Using the law of total probability:</p> \[\Pr[f(M(D)) \in S_0] = \int_{\mathcal{R}} \Pr[f(r) \in S_0] \cdot \Pr[M(D) = r] \, dr.\] <p>Similarly, for \(D'\)</p> \[\Pr[f(M(D')) \in S_0] = \int_{\mathcal{R}} \Pr[f(r) \in S_0] \cdot \Pr[M(D') = r] \, dr.\] <p>By the differential privacy assumption for \(\), we know:</p> \[\Pr[M(D) = r] \leq e^{\varepsilon} \Pr[M(D') = r] + \delta.\] <p>Multiplying both sides by \(\Pr[f(r) \in S_0]\) (which is always non-negative) and integrating:</p> \[\int_{\mathcal{R}} \Pr[f(r) \in S_0] \cdot \Pr[M(D) = r] \, dr \leq \int_{\mathcal{R}} \Pr[f(r) \in S_0] \cdot (e^{\varepsilon} \Pr[M(D') = r] + \delta) \, dr.\] <p>Since probabilities are at most 1, the last integral is at most 1, giving:</p> \[\Pr[f(M(D)) \in S_0] \leq e^{\varepsilon} \Pr[f(M(D')) \in S_0] + \delta.\] <p>This shows that applying \(\) to the output of a differentially private mechanism does not increase the distinguishability between \(M(D)\) and \(M(D')\). Since \(f\) does not have access to \(D\) itself, it cannot extract additional information beyond what is already present in \(M(D)\).</p> <h2 id="but-why-does-this-matter">But! Why Does This Matter?</h2> <p>This result, known as post-processing invariance, is crucial in privacy-preserving machine learning and data analysis because:</p> <ol> <li> <p>It allows us to safely process differentially private outputs: Once data has been privatized, further transformations (e.g., machine learning, reporting, visualization) do not weaken privacy.</p> </li> <li> <p>It simplifies DP system design: We don’t need to re-evaluate privacy guarantees at every stage if the input is DP, so is the output.</p> </li> <li> <p>It ensures privacy-preserving AI models remain secure: Even if a DP mechanism is used as input to a complex algorithm, the final result will still respect the original DP guarantees.</p> </li> </ol> <h2 id="whats-next">What’s Next?</h2> <p>In the upcoming blogs<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, we’ll explore different privacy-preserving mechanisms, such as the Laplace Mechanism and the Exponential Mechanism, diving into how they inject randomness to ensure differential privacy. We’ll break down their mathematical foundations, practical applications, and trade-offs, helping us understand when and how to use each effectively.</p> <hr/> <div style="text-align: center;"> <p><strong>Found this article helpful or have questions? 💡</strong></p> <p>I'm always happy to discuss Differential Privacy, answer your questions, or hear your feedback.</p> <p><strong><a href="mailto:divyanshu.singh.2019@gmail.com?subject=Discussion:%20Differential%20Privacy%20Blog%20Series">📧 Click here to send me an email</a></strong></p> </div> <hr/> <h3 id="references">References</h3> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Kumar, D. (2025). <a href="https://dvynsh.org/blog/2025/differential-privacy-but-why/">Differential Privacy!! But Why?</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>Kumar, D. (2025). <a href="https://github.com/divyanshugit/Inception-of-DP">Upcoming topics in this series</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="PETs"/><category term="Differnital Privacy"/><summary type="html"><![CDATA[Blog #2 in the series of Inception of Differential Privacy]]></summary></entry><entry><title type="html">Differential Privacy!! But Why?</title><link href="https://divyanshugit.github.io/blog/2025/differential-privacy-but-why/" rel="alternate" type="text/html" title="Differential Privacy!! But Why?"/><published>2025-02-16T00:00:00+00:00</published><updated>2025-02-16T00:00:00+00:00</updated><id>https://divyanshugit.github.io/blog/2025/differential-privacy-but-why</id><content type="html" xml:base="https://divyanshugit.github.io/blog/2025/differential-privacy-but-why/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dp_blog/1-480.webp 480w,/assets/img/dp_blog/1-800.webp 800w,/assets/img/dp_blog/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/dp_blog/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>There is no denying that data powering everything from AI models to decision making, the challenge is clear: how do we extract meaningful insights without compromising individual privacy? Whether it’s healthcare records, census data, or user behavior logs, the struggle remains the same balancing the value of data with the need to protect it.</p> <h2 id="the-core-dilemma-balancing-privacy-and-utility">The Core Dilemma: Balancing Privacy and Utility</h2> <p>Today’s adversaries are more sophisticated than ever. They harness advanced data mining techniques and merge diverse auxiliary sources like newspapers, medical studies, and labor statistics to piece together private information. Imagine a detective collecting small clues from various sources: individually, these clues may seem trivial, but together they can expose a complete portrait of someone’s personal data. Traditionally, there have been two extreme approaches to safeguard privacy:</p> <ul> <li> <p>Encryption: Excellent for maintaining secrecy, yet it converts data into ciphertext that is nearly impossible to analyze for useful patterns.</p> </li> <li> <p>Anonymization: Easier to implement but often vulnerable to “de-anonymization” attacks when adversaries have access to additional data sources.</p> </li> </ul> <blockquote> <p>A notable case is the Netflix Prize challenge, where anonymized movie ratings were cross-referenced with IMDb reviews, enabling <a href="https://www.cs.princeton.edu/~arvindn/">Prof Arvind Narayanan</a> and <a href="https://www.cs.cornell.edu/~shmat/">Prof Vitaly Shmatikov</a> to re-identify individuals. <a href="https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf">Know more</a></p> </blockquote> <p>Even though insights from private data are crucial for enhancing services and refining machine learning models, exposing personal details carries serious risks. Leaked health data, for example, can lead to discrimination, while biased training data might result in unfair decisions by AI systems. One way to quantify this risk is by considering the probability that an adversary can infer sensitive information from the output.</p> \[\text{Risk} = \Pr[\text{Adversary infers sensitive information} \mid \text{Output}]\] <p>To mitigate this risk, <strong><em>Differential Privacy (DP)</em></strong><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> comes in to rescue offering a way to learn from data without exposing personal information.</p> <h2 id="enter-differential-privacy-the-protective-mechanism">Enter Differential Privacy: The Protective Mechanism</h2> <p>DP ensures that the presence or absence of any single individual in a dataset has a negligible effect on the resulting analysis or model. Formally, a mechanism M satisfies (\(\epsilon, \delta\))-differential privacy if, for any two neighbouring datasets \(D\) and \(D'\) (differing by one record), and any subset of possible outputs \(S\):</p> \[\Pr[\mathcal{M}(D) \in S] \leq e^\epsilon \cdot \Pr[\mathcal{M}(D') \in S] + \delta\] <ul> <li> <p>\(\epsilon\) (privacy budget): Measures allowable privacy loss; smaller values imply stronger privacy.</p> </li> <li> <p>\(\delta\): Accommodates rare events where the strict guarantee might slightly lapse.</p> </li> </ul> <h3 id="to-understand-how-dp-works-we-need-to-explore-two-key-concepts">To understand how DP works, we need to explore two key concepts:</h3> <ul> <li> <p><strong>Sensitivity - Measuring the Impact of One Data Point:</strong> Consider a function \(f\) that processes a dataset \(D\) to produce some result (e.g., a statistical summary). Sensitivity quantifies the maximum change in the output when a single record is added or removed. It is defined as:</p> \[\Delta f = \max_{D, D'} \|f(D) - f(D')\|_1\] <p>where \(D\) and \(D'\) are neighboring datasets. Think of this as checking how much a single ingredient can alter the taste of a recipe—the lower the sensitivity, the less impact any single ingredient (or data point) has on the overall result.</p> </li> <li> <p><strong>Privacy Loss - Keeping the Secrets Safe:</strong> Now, consider comparing two nearly identical analyses—one with a particular individual’s data and one without. The privacy loss measures how much additional information the output reveals about that individual’s data. It is given by the privacy loss random variable:</p> \[\mathcal{L}_{\mathcal{M}}(D, D', o) = \log \left( \frac{\Pr[\mathcal{M}(D) = o]}{\Pr[\mathcal{M}(D') = o]} \right)\] <p>By controlling this value (keeping it bounded by \(\epsilon\)), DP ensures that the inclusion or exclusion of any single record does not significantly alter the output.</p> </li> </ul> <p>Together, sensitivity and privacy loss work as guardrails. They ensure that while we can still perform meaningful analysis, the influence of any individual data point is strictly limited protecting privacy without sacrificing the utility of the data. We’ll delve deeper into these concepts in upcoming posts.</p> <h2 id="differential-privacy-in-action">Differential Privacy in Action</h2> <p>DP isn’t just a theoretical construct; it’s making real-world impacts:</p> <ul> <li> <p>U.S. Census Bureau: DP is used to release statistical insights without exposing individual responses, protecting millions of respondents<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p> </li> <li> <p>Google’s Massive Deployment: DP is scaled across nearly three billion devices, safeguarding telemetry data in products like Google Home and Search while enabling useful analytics<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p> </li> <li> <p>Apple’s On-Device Analytics: DP is applied in iOS for collecting aggregate usage statistics (e.g., emoji usage, autocorrect patterns) without linking data to any individual user<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>.</p> </li> <li> <p>Betterdata’s Synthetic Data Solutions: Employs Differential Privacy to generate highly realistic synthetic data, enabling organizations to share and analyze information without exposing sensitive individual details<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p> </li> <li> <p>Sector Impact – Healthcare &amp; Finance: In healthcare, DP could have mitigated risks seen in major breaches (such as Change Healthcare) by ensuring that even aggregate models cannot be reverse-engineered to reveal individual patient data.</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Differential Privacy addresses a fundamental question in the digital age: <strong><em>How can we leverage complex, sensitive datasets to power AI and analytics without exposing individuals to risk?</em></strong> By weaving noise into computations and limiting how much a single record can influence the result, DP provides mathematically sound privacy assurances. This, in turn, unlocks a richer, more ethical world of data-driven discoveries especially as generative AI becomes ever more prevalent.</p> <p>So, next time you ponder the complexities of data analysis, remember: Differential Privacy isn’t just about complex equations—it’s about creating a safe and ethical pathway for innovation in the age of AI. In our next post, we’ll dive deep into the mathematics behind differential privacy and explore how noise injection mechanisms work to protect individual privacy while maintaining data utility<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>.</p> <hr/> <div style="text-align: center;"> <p><strong>Found this article helpful or have questions? 💡</strong></p> <p>I'm always happy to discuss Differential Privacy, answer your questions, or hear your feedback.</p> <p><strong><a href="mailto:divyanshu.singh.2019@gmail.com?subject=Discussion:%20Differential%20Privacy%20Blog%20Series">📧 Click here to send me an email</a></strong></p> </div> <hr/> <h3 id="references">References:</h3> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Dwork, C. (2006). <a href="https://www.comp.nus.edu.sg/~tankl/cs5322/readings/dwork.pdf">Differential Privacy</a>. International Colloquium on Automata, Languages, and Programming. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>Cohen, A., &amp; Nissim, K. (2020). <a href="https://mit-serc.pubpub.org/pub/differential-privacy-2020-us-census/release/2">Differential Privacy and the 2020 US Census</a>. MIT Science &amp; Technology Review. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>Guevara, M. (2024). <a href="https://www.helpnetsecurity.com/2024/10/31/miguel-guevara-google-implementing-differential-privacy/">Google on scaling differential privacy across nearly three billion devices</a>. Help Net Security. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4" role="doc-endnote"> <p>Apple Inc. (2023). <a href="https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf">Differential Privacy Overview</a>. Apple Privacy Documentation. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5" role="doc-endnote"> <p>Betterdata. (2024). <a href="https://www.betterdata.ai/blogs/differentially-private-synthetic-healthcare-data-for-better-insights">Differentially Private Synthetic Healthcare Data For Better Insights</a>. Betterdata Blog. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6" role="doc-endnote"> <p>Kumar, D. (202X). <a href="https://github.com/divyanshugit/Inception-of-DP">Upcoming topics in this series</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="PETs"/><category term="Differnital Privacy"/><summary type="html"><![CDATA[Blog #2 in the series of Inception of Differential Privacy]]></summary></entry><entry><title type="html">Exploring Llama.cpp with Llama Models</title><link href="https://divyanshugit.github.io/blog/2024/llm-quantization/" rel="alternate" type="text/html" title="Exploring Llama.cpp with Llama Models"/><published>2024-08-25T00:00:00+00:00</published><updated>2024-08-25T00:00:00+00:00</updated><id>https://divyanshugit.github.io/blog/2024/llm-quantization</id><content type="html" xml:base="https://divyanshugit.github.io/blog/2024/llm-quantization/"><![CDATA[<p>While thinking about what to do this weekend, I decided to revisit and update the paper <code class="language-plaintext highlighter-rouge">Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes</code> with the latest models and additional insights on quantization. As I dug deeper into new references, I realized that the vulnerability aspect of model quantization hasn’t been thoroughly explored. Previously, we used an off-the-shelf model to perform quick experiments, thanks to <a href="https://huggingface.co/TheBloke">The Bloke</a> . Now, let’s dive into the latest findings and learn more about how to quantize models.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llama_cpp-480.webp 480w,/assets/img/llama_cpp-800.webp 800w,/assets/img/llama_cpp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/llama_cpp.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="open-source-libraries-for-model-quantization">Open-Source Libraries for Model Quantization</h2> <p>During my research, I identified three standout open-source packages that are particularly effective for LLM quantization:</p> <ul> <li><strong>Llama.cpp</strong>: A versatile tool that quickly became my go-to solution.</li> <li><strong>GPTQ</strong>: Another robust option worth considering.</li> <li><strong>AWQ</strong>: Completes the trio with its unique strengths.</li> </ul> <p>I started with <strong>Llama.cpp</strong> and found it met all my requirements. So, I decided to move forward with this one. Let’s dive into how to set up and use Llama.cpp.</p> <h3 id="setting-up-llamacpp-locally">Setting Up Llama.cpp Locally</h3> <p>The first step in our journey is to set up Llama.cpp on your local machine. Start by cloning the repository and getting familiar with its structure:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ggerganov/llama.cpp.git
<span class="nb">cd</span> ~/llama.cpp
</code></pre></div></div> <p>Once the repository is cloned, you’ll need to install Llama.cpp locally. The following commands will ensure everything is set up correctly:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-e</span> <span class="nb">.</span>  <span class="c"># Installs Llama.cpp in editable mode</span>
pip <span class="nb">install</span> <span class="nt">-r</span> requirements/requirements-convert_hf_to_gguf.txt
</code></pre></div></div> <p>Now that the basic setup is complete, the next step is to build the necessary binaries for quantization. This is crucial for optimizing the models we’ll be working with:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make llama-quantize
</code></pre></div></div> <p>In addition to this, we’ll also build the command-line interface (CLI) tool, which will allow us to validate models quickly and efficiently:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make llama-cli
</code></pre></div></div> <p>With these tools in place, you’re now fully equipped to start experimenting with LLM quantization.</p> <p>We’ll start by targeting the following Llama models, known for their robustness against jailbreak attacks:</p> <ul> <li><strong>CodeLlama-7b</strong>: A highly capable model for code-related tasks.</li> <li><strong>Llama-2-7b-chat-hf</strong>: Excellent for conversational AI.</li> <li><strong>Llama-3-8b-instruct</strong>: A model designed for instructional tasks.</li> <li><strong>Llama-3.1-8b-instruct</strong>: An enhanced version of Llama-3, offering even greater capabilities.</li> </ul> <h3 id="downloading-and-preparing-model-weights">Downloading and Preparing Model Weights</h3> <p>To facilitate quick experimentation, I prefer to download model weights locally. This approach allows for faster processing and easier manipulation of the models. However, if you prefer, you can directly use the <code class="language-plaintext highlighter-rouge">--repo</code> flag in <code class="language-plaintext highlighter-rouge">llama-quantize</code> to work with models from the Hugging Face repository.</p> <p>Here’s a Python script to download the models:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim download_models.py
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">huggingface_hub</span> <span class="kn">import</span> <span class="n">snapshot_download</span>

<span class="n">model_ids</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">meta-llama/CodeLlama-7b-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-chat-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">meta-llama/Meta-Llama-3-8B-Instruct</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">meta-llama/Meta-Llama-3.1-8B-Instruct</span><span class="sh">"</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">model_id</span> <span class="ow">in</span> <span class="n">model_ids</span><span class="p">:</span>
    <span class="nf">snapshot_download</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
        <span class="n">local_dir</span><span class="o">=</span><span class="n">model_id</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">lower</span><span class="p">().</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">),</span>
        <span class="n">local_dir_use_symlinks</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <h3 id="converting-and-quantizing-models">Converting and Quantizing Models</h3> <p>After downloading the models, the next step is to convert them to the GGUF format, which is necessary for further quantization. We’ll start by converting the models to <code class="language-plaintext highlighter-rouge">fp16</code>, which stands for 16-bit floating point precision. To do this we can directly use the <code class="language-plaintext highlighter-rouge">conver_hf_to_gguf.py</code>. Though I’ve modified it to play around things that how much we can do with the python wrapper of it. If you’re interested then you can find it over <a href="https://github.com/divyanshugit/quantization/blob/main/convert_hf.py">here</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">convert_hf</span><span class="p">.</span><span class="n">py</span> <span class="n">meta_llama</span><span class="o">-</span><span class="mf">3.1_8</span><span class="n">b_instruct</span> <span class="o">--</span><span class="n">outfile</span> <span class="n">llama_3</span><span class="p">.</span><span class="mi">1</span><span class="n">_qf_16</span><span class="p">.</span><span class="n">gguf</span> <span class="o">--</span><span class="n">outtype</span> <span class="sh">"</span><span class="s">f16</span><span class="sh">"</span>
</code></pre></div></div> <p>Once the models are in GGUF format, we can proceed with quantizing them to 2-bit, 4-bit, and 8-bit representations. For instance, here’s how to convert the model to an 4-bit format:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./llama-quantize meta_llama_3.1_qf_16.gguf llama_3.1_8b_Q4_K_M.gguf Q4_K_M
</code></pre></div></div> <h3 id="validating-the-quantized-model">Validating the Quantized Model</h3> <p>With the quantized model in hand, it’s essential to validate its performance. The <code class="language-plaintext highlighter-rouge">llama-cli</code> tool we built earlier will come in handy for this task. Here’s a command to test the model and assess its speed and accuracy:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./lama-cli <span class="nt">-m</span> llama_3.1_8b_Q4_K_M.gguf <span class="nt">-p</span> <span class="s2">"You are a helpful assistant"</span> <span class="nt">-cnv</span>
</code></pre></div></div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/response-480.webp 480w,/assets/img/response-800.webp 800w,/assets/img/response-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/response.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="conclusion">Conclusion</h3> <p>Quantizing models is an effective strategy for optimizing them, particularly when dealing with limited resources. The tools and techniques we’ve explored provide a strong starting point for anyone interested in LLM quantization. For those curious to see the results in action, all the quantized models we discussed are available on Hugging Face in the Quantized-Llama Collection.</p> <p>Additionally, if you want; you can use <code class="language-plaintext highlighter-rouge">llama-cpp-python</code> for model inference, as <code class="language-plaintext highlighter-rouge">ctransformers</code> is currently not updated to support the latest model architectures.</p> <h3 id="references">References:</h3> <ul> <li><a href="https://github.com/ggerganov/llama.cpp">Llama.cpp</a></li> <li><a href="https://github.com/abetlen/llama-cpp-python">llama-cpp-python</a></li> <li><a href="https://arxiv.org/abs/2404.04392">Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes</a></li> </ul>]]></content><author><name></name></author><category term="language-models"/><category term="LLMs"/><category term="Quantization"/><summary type="html"><![CDATA[Quantizing models for fun.]]></summary></entry><entry><title type="html">🔍 InterrogateLLM: In Search of Truth</title><link href="https://divyanshugit.github.io/blog/2024/interrogate_llm/" rel="alternate" type="text/html" title="🔍 InterrogateLLM: In Search of Truth"/><published>2024-04-28T00:00:00+00:00</published><updated>2024-04-28T00:00:00+00:00</updated><id>https://divyanshugit.github.io/blog/2024/interrogate_llm</id><content type="html" xml:base="https://divyanshugit.github.io/blog/2024/interrogate_llm/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/interrogatellm-480.webp 480w,/assets/img/interrogatellm-800.webp 800w,/assets/img/interrogatellm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/interrogatellm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the world of LLMs, one big puzzle is hallucination. It’s when LLM makes up stuff that isn’t true, and it’s been confusing experts for a long time. This makes it hard to trust what LLM says. There is a new paper called <a href="https://arxiv.org/abs/2403.02889">InterrogateLLM</a> by <a href="https://itzikmalkiel.github.io/">Itzik Malkiel</a> and Yakir Yehuda that might help clear things up.</p> <p>To identify hallucination in an answer, it does something simple: it asks the model a bunch of times to recreate the original question using the answer it generated, much like SelfCheckGPT, which examines hallucination in the response. Then, InterrogateLLM measures how much the new versions of the question differ from the original one. When there’s a big difference, it suggests there might be a hallucination. Basically, if the model is making stuff up, it won’t be able to stick to the original question when asked to repeat it. This way of questioning forms the core of our method for finding hallucinations in answers.</p> <h2 id="heres-how-the-entire-process-works">Here’s how the entire process works</h2> <p><strong>Step 1: Generating Answers from Query</strong></p> <p>We start with asking the LLM to provide answers to a given question, saving the answers for later examination and reconstruction.</p> <p><strong>Step 2: Reconstructing Queries from Answers</strong></p> <p>This is where the real magic happens. Building on the answers from the previous step, InterrogateLLM uses a backward process to piece together the original question. By carefully comparing the generated answers with the intended question, the system reconstructs what was initially asked.</p> <p><strong>Step 3: Generating Text Embeddings of Queries &amp; Reconstructed Queries</strong></p> <p>Now, with both the original question and its reconstructed version on hand, InterrogateLLM creates text embeddings for each. Text embeddings transform textual information into high-dimensional vectors, making it easier to compare and analyze them.</p> <p><strong>Final Step: Predicting Hallucinations with SBERT</strong></p> <p>The final piece of the puzzle involves using SBERT(Sentence-BERT). By comparing the text embeddings of the original question and its reconstruction, SBERT determines if there’s any hallucination. If the similarity is below a certain threshold, suggesting significant deviation between the two, it raises a flag for potential hallucination.</p> <p>While not a complete solution to the hallucination problem, InterrogateLLM represents a promising step toward more reliable and trustworthy language AI systems. Where it provides a systematic framework for identifying hallucination in any domain by assessing the discrepancy between the intended query and the generated response. As research in this area progresses, we can expect further improvements and innovations to solve the problem of Hallucinations.</p>]]></content><author><name></name></author><category term="language-models"/><category term="LLMs"/><category term="Hallucinations"/><summary type="html"><![CDATA[Explore how InterrogateLLM addresses AI hallucination in a straightforward manner.]]></summary></entry></feed>