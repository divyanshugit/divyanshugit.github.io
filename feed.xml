<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://divyanshugit.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://divyanshugit.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-11T06:36:54+00:00</updated><id>https://divyanshugit.github.io/feed.xml</id><title type="html">Divyanshu Kumar</title><subtitle>I&apos;m a ML Reserach Engineer at Enkrypt AI, where I work on finding vulnerability of Gen AI models and developing tools to make the Gen AI usecase safe and secure. </subtitle><entry><title type="html">üîç InterrogateLLM: In Search of Truth</title><link href="https://divyanshugit.github.io/blog/2024/interrogate_llm/" rel="alternate" type="text/html" title="üîç InterrogateLLM: In Search of Truth"/><published>2024-04-28T00:00:00+00:00</published><updated>2024-04-28T00:00:00+00:00</updated><id>https://divyanshugit.github.io/blog/2024/interrogate_llm</id><content type="html" xml:base="https://divyanshugit.github.io/blog/2024/interrogate_llm/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/interrogatellm-480.webp 480w,/assets/img/interrogatellm-800.webp 800w,/assets/img/interrogatellm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/interrogatellm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the world of LLMs, one big puzzle is hallucination. It‚Äôs when LLM makes up stuff that isn‚Äôt true, and it‚Äôs been confusing experts for a long time. This makes it hard to trust what LLM says. There is a new paper called <a href="https://arxiv.org/abs/2403.02889">InterrogateLLM</a> by <a href="https://itzikmalkiel.github.io/">Itzik Malkiel</a> and Yakir Yehuda that might help clear things up.</p> <p>To identify hallucination in an answer, it does something simple: it asks the model a bunch of times to recreate the original question using the answer it generated, much like SelfCheckGPT, which examines hallucination in the response. Then, InterrogateLLM measures how much the new versions of the question differ from the original one. When there‚Äôs a big difference, it suggests there might be a hallucination. Basically, if the model is making stuff up, it won‚Äôt be able to stick to the original question when asked to repeat it. This way of questioning forms the core of our method for finding hallucinations in answers.</p> <h2 id="heres-how-the-entire-process-works">Here‚Äôs how the entire process works</h2> <p><strong>Step 1: Generating Answers from Query</strong></p> <p>We start with asking the LLM to provide answers to a given question, saving the answers for later examination and reconstruction.</p> <p><strong>Step 2: Reconstructing Queries from Answers</strong></p> <p>This is where the real magic happens. Building on the answers from the previous step, InterrogateLLM uses a backward process to piece together the original question. By carefully comparing the generated answers with the intended question, the system reconstructs what was initially asked.</p> <p><strong>Step 3: Generating Text Embeddings of Queries &amp; Reconstructed Queries</strong></p> <p>Now, with both the original question and its reconstructed version on hand, InterrogateLLM creates text embeddings for each. Text embeddings transform textual information into high-dimensional vectors, making it easier to compare and analyze them.</p> <p><strong>Final Step: Predicting Hallucinations with SBERT</strong></p> <p>The final piece of the puzzle involves using SBERT(Sentence-BERT). By comparing the text embeddings of the original question and its reconstruction, SBERT determines if there‚Äôs any hallucination. If the similarity is below a certain threshold, suggesting significant deviation between the two, it raises a flag for potential hallucination.</p> <p>While not a complete solution to the hallucination problem, InterrogateLLM represents a promising step toward more reliable and trustworthy language AI systems. Where it provides a systematic framework for identifying hallucination in any domain by assessing the discrepancy between the intended query and the generated response. As research in this area progresses, we can expect further improvements and innovations to solve the problem of Hallucinations.</p>]]></content><author><name></name></author><category term="language-models"/><category term="LLMs"/><category term="Hallucinations"/><summary type="html"><![CDATA[Explore how InterrogateLLM addresses AI hallucination in a straightforward manner.]]></summary></entry></feed>