<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Exploring Llama.cpp with Llama Models | Divyanshu Kumar </title> <meta name="author" content="Divyanshu Kumar"> <meta name="description" content="Quantizing models for fun."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.svg?d10861c21807364ff037bb330c80684d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://divyanshugit.github.io/blog/2024/llm-quantization/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Divyanshu Kumar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Exploring Llama.cpp with Llama Models</h1> <p class="post-meta"> Created in August 25, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> LLMs</a>   <a href="/blog/tag/quantization"> <i class="fa-solid fa-hashtag fa-sm"></i> Quantization</a>     ·   <a href="/blog/category/language-models"> <i class="fa-solid fa-tag fa-sm"></i> language-models</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>While thinking about what to do this weekend, I decided to revisit and update the paper <code class="language-plaintext highlighter-rouge">Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes</code> with the latest models and additional insights on quantization. As I dug deeper into new references, I realized that the vulnerability aspect of model quantization hasn’t been thoroughly explored. Previously, we used an off-the-shelf model to perform quick experiments, thanks to <a href="https://huggingface.co/TheBloke" rel="external nofollow noopener" target="_blank">The Bloke</a> . Now, let’s dive into the latest findings and learn more about how to quantize models.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llama_cpp-480.webp 480w,/assets/img/llama_cpp-800.webp 800w,/assets/img/llama_cpp-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/llama_cpp.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="open-source-libraries-for-model-quantization">Open-Source Libraries for Model Quantization</h2> <p>During my research, I identified three standout open-source packages that are particularly effective for LLM quantization:</p> <ul> <li> <strong>Llama.cpp</strong>: A versatile tool that quickly became my go-to solution.</li> <li> <strong>GPTQ</strong>: Another robust option worth considering.</li> <li> <strong>AWQ</strong>: Completes the trio with its unique strengths.</li> </ul> <p>I started with <strong>Llama.cpp</strong> and found it met all my requirements. So, I decided to move forward with this one. Let’s dive into how to set up and use Llama.cpp.</p> <h3 id="setting-up-llamacpp-locally">Setting Up Llama.cpp Locally</h3> <p>The first step in our journey is to set up Llama.cpp on your local machine. Start by cloning the repository and getting familiar with its structure:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ggerganov/llama.cpp.git
<span class="nb">cd</span> ~/llama.cpp
</code></pre></div></div> <p>Once the repository is cloned, you’ll need to install Llama.cpp locally. The following commands will ensure everything is set up correctly:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-e</span> <span class="nb">.</span>  <span class="c"># Installs Llama.cpp in editable mode</span>
pip <span class="nb">install</span> <span class="nt">-r</span> requirements/requirements-convert_hf_to_gguf.txt
</code></pre></div></div> <p>Now that the basic setup is complete, the next step is to build the necessary binaries for quantization. This is crucial for optimizing the models we’ll be working with:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make llama-quantize
</code></pre></div></div> <p>In addition to this, we’ll also build the command-line interface (CLI) tool, which will allow us to validate models quickly and efficiently:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make llama-cli
</code></pre></div></div> <p>With these tools in place, you’re now fully equipped to start experimenting with LLM quantization.</p> <p>We’ll start by targeting the following Llama models, known for their robustness against jailbreak attacks:</p> <ul> <li> <strong>CodeLlama-7b</strong>: A highly capable model for code-related tasks.</li> <li> <strong>Llama-2-7b-chat-hf</strong>: Excellent for conversational AI.</li> <li> <strong>Llama-3-8b-instruct</strong>: A model designed for instructional tasks.</li> <li> <strong>Llama-3.1-8b-instruct</strong>: An enhanced version of Llama-3, offering even greater capabilities.</li> </ul> <h3 id="downloading-and-preparing-model-weights">Downloading and Preparing Model Weights</h3> <p>To facilitate quick experimentation, I prefer to download model weights locally. This approach allows for faster processing and easier manipulation of the models. However, if you prefer, you can directly use the <code class="language-plaintext highlighter-rouge">--repo</code> flag in <code class="language-plaintext highlighter-rouge">llama-quantize</code> to work with models from the Hugging Face repository.</p> <p>Here’s a Python script to download the models:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim download_models.py
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">huggingface_hub</span> <span class="kn">import</span> <span class="n">snapshot_download</span>

<span class="n">model_ids</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">meta-llama/CodeLlama-7b-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-chat-hf</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">meta-llama/Meta-Llama-3-8B-Instruct</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">meta-llama/Meta-Llama-3.1-8B-Instruct</span><span class="sh">"</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">model_id</span> <span class="ow">in</span> <span class="n">model_ids</span><span class="p">:</span>
    <span class="nf">snapshot_download</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
        <span class="n">local_dir</span><span class="o">=</span><span class="n">model_id</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">lower</span><span class="p">().</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">),</span>
        <span class="n">local_dir_use_symlinks</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <h3 id="converting-and-quantizing-models">Converting and Quantizing Models</h3> <p>After downloading the models, the next step is to convert them to the GGUF format, which is necessary for further quantization. We’ll start by converting the models to <code class="language-plaintext highlighter-rouge">fp16</code>, which stands for 16-bit floating point precision. To do this we can directly use the <code class="language-plaintext highlighter-rouge">conver_hf_to_gguf.py</code>. Though I’ve modified it play around things that how much we can do with the python wrapper of it. If you’re interested then you can find it over <a href="https://github.com/divyanshugit/quantization/convert_hf.py" rel="external nofollow noopener" target="_blank">here</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">convert_hf</span><span class="p">.</span><span class="n">py</span> <span class="n">meta_llama</span><span class="o">-</span><span class="mf">3.1_8</span><span class="n">b_instruct</span> <span class="o">--</span><span class="n">outfile</span> <span class="n">llama_3</span><span class="p">.</span><span class="mi">1</span><span class="n">_qf_16</span><span class="p">.</span><span class="n">gguf</span> <span class="o">--</span><span class="n">outtype</span> <span class="sh">"</span><span class="s">f16</span><span class="sh">"</span>
</code></pre></div></div> <p>Once the models are in GGUF format, we can proceed with quantizing them to 2-bit, 4-bit, and 8-bit representations. For instance, here’s how to convert the model to an 4-bit format:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./llama-quantize meta_llama_3.1_qf_16.gguf llama_3.1_8b_Q4_K_M.gguf Q4_K_M
</code></pre></div></div> <h3 id="validating-the-quantized-model">Validating the Quantized Model</h3> <p>With the quantized model in hand, it’s essential to validate its performance. The <code class="language-plaintext highlighter-rouge">llama-cli</code> tool we built earlier will come in handy for this task. Here’s a command to test the model and assess its speed and accuracy:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./lama-cli <span class="nt">-m</span> llama_3.1_8b_Q4_K_M.gguf <span class="nt">-p</span> <span class="s2">"You are a helpful assistant"</span> <span class="nt">-cnv</span>
</code></pre></div></div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/response-480.webp 480w,/assets/img/response-800.webp 800w,/assets/img/response-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/response.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="conclusion">Conclusion</h3> <p>Quantizing models is an effective strategy for optimizing them, particularly when dealing with limited resources. The tools and techniques we’ve explored provide a strong starting point for anyone interested in LLM quantization. For those curious to see the results in action, all the quantized models we discussed are available on Hugging Face in the Quantized-Llama Collection.</p> <p>Additionally, if you want; you can use <code class="language-plaintext highlighter-rouge">llama-cpp-python</code> for model inference, as <code class="language-plaintext highlighter-rouge">ctransformers</code> is currently not updated to support the latest model architectures.</p> <h3 id="references">References:</h3> <ul> <li><a href="https://github.com/ggerganov/llama.cpp" rel="external nofollow noopener" target="_blank">Llama.cpp</a></li> <li><a href="https://github.com/abetlen/llama-cpp-python" rel="external nofollow noopener" target="_blank">llama-cpp-python</a></li> <li><a href="https://arxiv.org/abs/2404.04392" rel="external nofollow noopener" target="_blank">Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes</a></li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/interrogate_llm/">🔍 InterrogateLLM: In Search of Truth</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Divyanshu Kumar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>